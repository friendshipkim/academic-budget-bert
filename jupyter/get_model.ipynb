{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "# from ../pretraining/base.py import BasePretrainModel\n",
    "from pretraining.base import BasePretrainModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1306334327.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [15]\u001b[0;36m\u001b[0m\n\u001b[0;31m    Namespace({deepspeed=True, deepspeed_transformer_kernel=False, stochastic_mode=False, attention_dropout_checkpoint=False, normalize_invertible=False, gelu_checkpoint=False, gradient_clipping=0.0, steps_per_print=100, wall_clock_breakdown=False, prescale_gradients=False, gradient_predivide_factor=None, fp16=True, fp16_backend='ds', fp16_opt='O2', model_type='bert-mlm', model_name_or_path=None, config_name=None, tokenizer_name='bert-large-uncased', load_tokenizer_locally=False, cache_dir=None, pretrain_run_args=None, dataset_path='/opt/ml/data/total/', num_workers=4, async_worker=True, data_loader_type='dist', seed=42, output_dir='/opt/ml/data/saved_models/4stitch', max_predictions_per_seq=20, local_rank=0, load_training_checkpoint=None, load_checkpoint_id=None, num_epochs_between_checkpoints=10, job_name='4xhalflarge-total', project_name='budget-bert-pretraining', max_steps=500, max_steps_per_epoch=9223372036854775807, print_steps=100, do_validation=True, validation_epochs=3, validation_epochs_begin=1, validation_epochs_end=1, validation_begin_proportion=0.05, validation_end_proportion=0.01, validation_micro_batch=16, validation_shards=1, add_nsp=False, current_run_id='total', early_exit_time_marker=100.0, total_training_time=100.0, finetune_time_markers=None, finetune_checkpoint_at_end=True, gradient_accumulation_steps=1, train_batch_size=4096, train_micro_batch_size_per_gpu=256, num_epochs=1000000, lr=0.001, use_early_stopping=False, early_stop_time=720, early_stop_eval_loss=2.1, scale_cnt_limit=100, log_throughput_every=20, no_nsp=True, learning_rate=0.001, do_stitch=True, src_model1_path='/opt/ml/data/saved_models/halflarge_213-set1-10ksteps/set1-10ksteps/epoch1000000_step10023/', src_model2_path='/opt/ml/data/saved_models/halflarge_146-set1-10ksteps/set1-10ksteps/epoch1000000_step10031/', src_model3_path='/opt/ml/data/saved_models/halflarge_95-set1-10ksteps/set1-10ksteps/epoch1000000_step10005/', src_model4_path='/opt/ml/data/saved_models/halflarge_199-set1-10ksteps/set1-10ksteps/epoch1000000_step10014/', skip_layernorm=False, model_config={'vocab_size': 30522, 'hidden_size': 512, 'num_hidden_layers': 24, 'num_attention_heads': 8, 'intermediate_size': 2048, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'layernorm_embedding': False, 'type_vocab_size': 2, 'initializer_range': 0.02, 'fused_linear_layer': True, 'sparse_mask_prediction': True, 'encoder_ln_mode': 'pre-ln', 'layer_norm_type': 'apex'}, optimizer_args=OptimizerArguments(optimizer_type='adamw', weight_decay=0.01, bias_correction=False, max_coeff=0.3, min_coeff=0.01, adam_beta1=0.9, adam_beta2=0.98, adam_eps=1e-06), schedule_args=SchedulerArgs(lr_schedule='constant_step', curve='linear', warmup_proportion=0.06, decay_rate=0.99, decay_step=1000, num_warmup_steps=100), logger=<pretraining.utils.Logger object at 0x7f542833e610>, ds_config={'train_batch_size': 4096, 'train_micro_batch_size_per_gpu': 256, 'steps_per_print': 100, 'gradient_clipping': 0.0, 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'min_loss_scale': 1, 'loss_scale_window': 1000, 'hysteresis': 2}}, deepspeed_config={'train_batch_size': 4096, 'train_micro_batch_size_per_gpu': 256, 'steps_per_print': 100, 'gradient_clipping': 0.0, 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'min_loss_scale': 1, 'loss_scale_window': 1000, 'hysteresis': 2}, saved_model_path:'/opt/ml/data/saved_models/4stitch/4xhalflarge-total/total', vocab_size:30528}})\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# import Namespace\n",
    "from argparse import Namespace\n",
    "Namespace({deepspeed=True, deepspeed_transformer_kernel=False, stochastic_mode=False, attention_dropout_checkpoint=False, normalize_invertible=False, gelu_checkpoint=False, gradient_clipping=0.0, steps_per_print=100, wall_clock_breakdown=False, prescale_gradients=False, gradient_predivide_factor=None, fp16=True, fp16_backend='ds', fp16_opt='O2', model_type='bert-mlm', model_name_or_path=None, config_name=None, tokenizer_name='bert-large-uncased', load_tokenizer_locally=False, cache_dir=None, pretrain_run_args=None, dataset_path='/opt/ml/data/total/', num_workers=4, async_worker=True, data_loader_type='dist', seed=42, output_dir='/opt/ml/data/saved_models/4stitch', max_predictions_per_seq=20, local_rank=0, load_training_checkpoint=None, load_checkpoint_id=None, num_epochs_between_checkpoints=10, job_name='4xhalflarge-total', project_name='budget-bert-pretraining', max_steps=500, max_steps_per_epoch=9223372036854775807, print_steps=100, do_validation=True, validation_epochs=3, validation_epochs_begin=1, validation_epochs_end=1, validation_begin_proportion=0.05, validation_end_proportion=0.01, validation_micro_batch=16, validation_shards=1, add_nsp=False, current_run_id='total', early_exit_time_marker=100.0, total_training_time=100.0, finetune_time_markers=None, finetune_checkpoint_at_end=True, gradient_accumulation_steps=1, train_batch_size=4096, train_micro_batch_size_per_gpu=256, num_epochs=1000000, lr=0.001, use_early_stopping=False, early_stop_time=720, early_stop_eval_loss=2.1, scale_cnt_limit=100, log_throughput_every=20, no_nsp=True, learning_rate=0.001, do_stitch=True, src_model1_path='/opt/ml/data/saved_models/halflarge_213-set1-10ksteps/set1-10ksteps/epoch1000000_step10023/', src_model2_path='/opt/ml/data/saved_models/halflarge_146-set1-10ksteps/set1-10ksteps/epoch1000000_step10031/', src_model3_path='/opt/ml/data/saved_models/halflarge_95-set1-10ksteps/set1-10ksteps/epoch1000000_step10005/', src_model4_path='/opt/ml/data/saved_models/halflarge_199-set1-10ksteps/set1-10ksteps/epoch1000000_step10014/', skip_layernorm=False, model_config={'vocab_size': 30522, 'hidden_size': 512, 'num_hidden_layers': 24, 'num_attention_heads': 8, 'intermediate_size': 2048, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'layernorm_embedding': False, 'type_vocab_size': 2, 'initializer_range': 0.02, 'fused_linear_layer': True, 'sparse_mask_prediction': True, 'encoder_ln_mode': 'pre-ln', 'layer_norm_type': 'apex'}, optimizer_args=OptimizerArguments(optimizer_type='adamw', weight_decay=0.01, bias_correction=False, max_coeff=0.3, min_coeff=0.01, adam_beta1=0.9, adam_beta2=0.98, adam_eps=1e-06), schedule_args=SchedulerArgs(lr_schedule='constant_step', curve='linear', warmup_proportion=0.06, decay_rate=0.99, decay_step=1000, num_warmup_steps=100), logger=<pretraining.utils.Logger object at 0x7f542833e610>, ds_config={'train_batch_size': 4096, 'train_micro_batch_size_per_gpu': 256, 'steps_per_print': 100, 'gradient_clipping': 0.0, 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'min_loss_scale': 1, 'loss_scale_window': 1000, 'hysteresis': 2}}, deepspeed_config={'train_batch_size': 4096, 'train_micro_batch_size_per_gpu': 256, 'steps_per_print': 100, 'gradient_clipping': 0.0, 'wall_clock_breakdown': False, 'fp16': {'enabled': True, 'loss_scale': 0, 'min_loss_scale': 1, 'loss_scale_window': 1000, 'hysteresis': 2}, saved_model_path:'/opt/ml/data/saved_models/4stitch/4xhalflarge-total/total', vocab_size:30528}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(model_config={'vocab_size': 30522, 'hidden_size': 512, 'num_hidden_layers': 24, 'num_attention_heads': 8, 'intermediate_size': 2048, 'hidden_act': 'gelu', 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'max_position_embeddings': 512, 'layernorm_embedding': False, 'type_vocab_size': 2, 'initializer_range': 0.02, 'fused_linear_layer': True, 'sparse_mask_prediction': True, 'encoder_ln_mode': 'pre-ln', 'layer_norm_type': 'apex'}\n",
    "deepspeed=True, deepspeed_transformer_kernel=False, stochastic_mode=False, attention_dropout_checkpoint=False, normalize_invertible=False, gelu_checkpoint=False, gradient_clipping=0.0, steps_per_print=100, wall_clock_breakdown=False, prescale_gradients=False, gradient_predivide_factor=None, fp16=True, fp16_backend='ds', fp16_opt='O2', model_type='bert-mlm', model_name_or_path=None, config_name=None, tokenizer_name='bert-large-uncased', load_tokenizer_locally=False, cache_dir=None, pretrain_run_args=None, dataset_path='/opt/ml/data/total/', num_workers=4, async_worker=True, data_loader_type='dist', seed=42, output_dir='/opt/ml/data/saved_models/4stitch', max_predictions_per_seq=20, local_rank=0, load_training_checkpoint=None, load_checkpoint_id=None, num_epochs_between_checkpoints=10, job_name='4xhalflarge-total', project_name='budget-bert-pretraining', max_steps=500, max_steps_per_epoch=9223372036854775807, print_steps=100, do_validation=True, validation_epochs=3, validation_epochs_begin=1, validation_epochs_end=1, validation_begin_proportion=0.05, validation_end_proportion=0.01, validation_micro_batch=16, validation_shards=1, add_nsp=False, current_run_id='total', early_exit_time_marker=100.0, total_training_time=100.0, finetune_time_markers=None, finetune_checkpoint_at_end=True, gradient_accumulation_steps=1, train_batch_size=4096, train_micro_batch_size_per_gpu=256, num_epochs=1000000, lr=0.001, use_early_stopping=False, early_stop_time=720, early_stop_eval_loss=2.1, scale_cnt_limit=100, log_throughput_every=20, no_nsp=True, learning_rate=0.001, do_stitch=True, src_model1_path='/opt/ml/data/saved_models/halflarge_213-set1-10ksteps/set1-10ksteps/epoch1000000_step10023/', src_model2_path='/opt/ml/data/saved_models/halflarge_146-set1-10ksteps/set1-10ksteps/epoch1000000_step10031/', src_model3_path='/opt/ml/data/saved_models/halflarge_95-set1-10ksteps/set1-10ksteps/epoch1000000_step10005/', src_model4_path='/opt/ml/data/saved_models/halflarge_199-set1-10ksteps/set1-10ksteps/epoch1000000_step10014/', skip_layernorm=False, saved_model_path='/opt/ml/data/saved_models/4stitch/4xhalflarge-total/total', exp_start_marker=21446606.92100428, vocab_size=30528, model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2023 22:35:42 - INFO - pretraining.base -   Loading default tokenizer bert-large-uncased\n",
      "02/19/2023 22:35:45 - INFO - pretraining.base -   Loading config from args\n",
      "02/19/2023 22:35:45 - INFO - pretraining.base -   VOCAB SIZE: 30528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "{}\n",
      "()\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2023 22:35:45 - INFO - pretraining.modeling -   Init BERT pretrain model\n",
      "02/19/2023 22:35:46 - INFO - pretraining.base -   Loading default tokenizer bert-large-uncased\n",
      "02/19/2023 22:35:49 - INFO - pretraining.base -   Loading config from args\n",
      "02/19/2023 22:35:49 - INFO - pretraining.base -   VOCAB SIZE: 30528\n",
      "02/19/2023 22:35:49 - INFO - pretraining.modeling -   Init BERT pretrain model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "{}\n",
      "()\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2023 22:35:50 - INFO - pretraining.base -   Loading default tokenizer bert-large-uncased\n",
      "02/19/2023 22:35:53 - INFO - pretraining.base -   Loading config from args\n",
      "02/19/2023 22:35:53 - INFO - pretraining.base -   VOCAB SIZE: 30528\n",
      "02/19/2023 22:35:53 - INFO - pretraining.modeling -   Init BERT pretrain model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "{}\n",
      "()\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2023 22:35:53 - INFO - pretraining.base -   Loading default tokenizer bert-large-uncased\n",
      "02/19/2023 22:35:56 - INFO - pretraining.base -   Loading config from args\n",
      "02/19/2023 22:35:56 - INFO - pretraining.base -   VOCAB SIZE: 30528\n",
      "02/19/2023 22:35:57 - INFO - pretraining.modeling -   Init BERT pretrain model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "{}\n",
      "()\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2023 22:35:57 - INFO - pretraining.base -   Loading default tokenizer bert-large-uncased\n",
      "02/19/2023 22:36:00 - INFO - pretraining.base -   Loading config from args\n",
      "02/19/2023 22:36:00 - INFO - pretraining.base -   VOCAB SIZE: 30528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "{}\n",
      "()\n",
      "{}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/19/2023 22:36:01 - INFO - pretraining.modeling -   Init BERT pretrain model\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BasePretrainModel' object has no attribute 'named_parameters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/code/jupyter/get_model.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f706564616e7469635f6b656c64797368222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d65677075613130302d613031227d7d/opt/ml/code/jupyter/get_model.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m not_freeze_types \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mlayernorm\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39membeddings\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f706564616e7469635f6b656c64797368222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d65677075613130302d613031227d7d/opt/ml/code/jupyter/get_model.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m net \u001b[39m=\u001b[39m stitched_model\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f706564616e7469635f6b656c64797368222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d65677075613130302d613031227d7d/opt/ml/code/jupyter/get_model.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m net\u001b[39m.\u001b[39;49mnamed_parameters():\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f706564616e7469635f6b656c64797368222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d65677075613130302d613031227d7d/opt/ml/code/jupyter/get_model.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     lower_name \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mlower()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f706564616e7469635f6b656c64797368222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d65677075613130302d613031227d7d/opt/ml/code/jupyter/get_model.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m([t \u001b[39min\u001b[39;00m lower_name \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m freeze_types]):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BasePretrainModel' object has no attribute 'named_parameters'"
     ]
    }
   ],
   "source": [
    "src_model1 = BasePretrainModel(args)\n",
    "src_model2 = BasePretrainModel(args)\n",
    "\n",
    "# checkpoint: OrderedDict with model params\n",
    "checkpoint1 = torch.load(args.src_model1_path + \"pytorch_model.bin\")\n",
    "src_model1.network.load_state_dict(checkpoint1)\n",
    "\n",
    "checkpoint2 = torch.load(args.src_model2_path + \"pytorch_model.bin\")\n",
    "src_model2.network.load_state_dict(checkpoint2)\n",
    "\n",
    "# stitch 4 models\n",
    "if True:\n",
    "    src_model3 = BasePretrainModel(args)\n",
    "    src_model4 = BasePretrainModel(args)\n",
    "\n",
    "    # checkpoint: OrderedDict with model params\n",
    "    checkpoint3 = torch.load(args.src_model3_path + \"pytorch_model.bin\")\n",
    "    src_model3.network.load_state_dict(checkpoint3)\n",
    "\n",
    "    checkpoint4 = torch.load(args.src_model4_path + \"pytorch_model.bin\")\n",
    "    src_model4.network.load_state_dict(checkpoint4)\n",
    "\n",
    "# # load the last checkpoint\n",
    "# last_checkpoint_path1 = '/n/tata_ddos_ceph/woojeong/saved_models/pretrain/training-out-halflarge/halflarge_pretraining-0/0/latest_checkpoint/mp_rank_00_model_states.pt'\n",
    "# last_checkpoint = torch.load(last_checkpoint_path1)['module']\n",
    "\n",
    "# define stitched model skeleton\n",
    "stitched_model = BasePretrainModel(args, model_type=\"stitched-bert-mlm\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not freezing bert.embeddings.word_embeddings.weight\n",
      "Not freezing bert.embeddings.position_embeddings.weight\n",
      "Not freezing bert.embeddings.token_type_embeddings.weight\n",
      "Not freezing bert.encoder.FinalLayerNorm.weight\n",
      "Not freezing bert.encoder.FinalLayerNorm.bias\n",
      "Freezing bert.encoder.layer.0.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.0.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.0.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.0.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.0.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.0.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.0.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.0.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.0.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.0.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.0.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.0.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.0.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.0.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.0.output.dense.weight\n",
      "Freezing bert.encoder.layer.0.output.dense.bias\n",
      "Freezing bert.encoder.layer.1.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.1.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.1.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.1.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.1.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.1.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.1.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.1.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.1.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.1.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.1.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.1.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.1.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.1.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.1.output.dense.weight\n",
      "Freezing bert.encoder.layer.1.output.dense.bias\n",
      "Freezing bert.encoder.layer.2.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.2.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.2.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.2.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.2.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.2.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.2.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.2.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.2.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.2.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.2.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.2.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.2.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.2.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.2.output.dense.weight\n",
      "Freezing bert.encoder.layer.2.output.dense.bias\n",
      "Freezing bert.encoder.layer.3.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.3.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.3.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.3.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.3.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.3.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.3.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.3.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.3.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.3.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.3.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.3.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.3.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.3.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.3.output.dense.weight\n",
      "Freezing bert.encoder.layer.3.output.dense.bias\n",
      "Freezing bert.encoder.layer.4.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.4.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.4.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.4.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.4.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.4.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.4.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.4.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.4.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.4.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.4.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.4.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.4.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.4.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.4.output.dense.weight\n",
      "Freezing bert.encoder.layer.4.output.dense.bias\n",
      "Freezing bert.encoder.layer.5.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.5.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.5.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.5.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.5.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.5.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.5.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.5.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.5.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.5.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.5.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.5.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.5.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.5.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.5.output.dense.weight\n",
      "Freezing bert.encoder.layer.5.output.dense.bias\n",
      "Freezing bert.encoder.layer.6.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.6.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.6.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.6.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.6.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.6.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.6.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.6.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.6.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.6.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.6.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.6.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.6.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.6.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.6.output.dense.weight\n",
      "Freezing bert.encoder.layer.6.output.dense.bias\n",
      "Freezing bert.encoder.layer.7.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.7.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.7.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.7.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.7.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.7.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.7.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.7.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.7.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.7.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.7.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.7.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.7.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.7.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.7.output.dense.weight\n",
      "Freezing bert.encoder.layer.7.output.dense.bias\n",
      "Freezing bert.encoder.layer.8.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.8.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.8.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.8.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.8.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.8.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.8.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.8.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.8.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.8.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.8.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.8.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.8.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.8.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.8.output.dense.weight\n",
      "Freezing bert.encoder.layer.8.output.dense.bias\n",
      "Freezing bert.encoder.layer.9.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.9.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.9.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.9.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.9.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.9.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.9.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.9.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.9.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.9.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.9.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.9.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.9.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.9.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.9.output.dense.weight\n",
      "Freezing bert.encoder.layer.9.output.dense.bias\n",
      "Freezing bert.encoder.layer.10.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.10.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.10.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.10.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.10.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.10.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.10.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.10.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.10.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.10.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.10.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.10.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.10.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.10.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.10.output.dense.weight\n",
      "Freezing bert.encoder.layer.10.output.dense.bias\n",
      "Freezing bert.encoder.layer.11.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.11.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.11.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.11.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.11.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.11.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.11.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.11.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.11.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.11.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.11.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.11.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.11.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.11.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.11.output.dense.weight\n",
      "Freezing bert.encoder.layer.11.output.dense.bias\n",
      "Freezing bert.encoder.layer.12.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.12.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.12.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.12.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.12.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.12.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.12.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.12.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.12.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.12.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.12.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.12.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.12.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.12.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.12.output.dense.weight\n",
      "Freezing bert.encoder.layer.12.output.dense.bias\n",
      "Freezing bert.encoder.layer.13.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.13.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.13.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.13.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.13.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.13.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.13.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.13.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.13.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.13.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.13.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.13.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.13.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.13.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.13.output.dense.weight\n",
      "Freezing bert.encoder.layer.13.output.dense.bias\n",
      "Freezing bert.encoder.layer.14.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.14.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.14.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.14.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.14.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.14.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.14.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.14.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.14.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.14.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.14.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.14.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.14.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.14.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.14.output.dense.weight\n",
      "Freezing bert.encoder.layer.14.output.dense.bias\n",
      "Freezing bert.encoder.layer.15.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.15.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.15.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.15.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.15.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.15.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.15.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.15.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.15.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.15.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.15.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.15.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.15.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.15.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.15.output.dense.weight\n",
      "Freezing bert.encoder.layer.15.output.dense.bias\n",
      "Freezing bert.encoder.layer.16.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.16.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.16.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.16.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.16.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.16.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.16.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.16.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.16.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.16.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.16.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.16.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.16.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.16.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.16.output.dense.weight\n",
      "Freezing bert.encoder.layer.16.output.dense.bias\n",
      "Freezing bert.encoder.layer.17.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.17.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.17.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.17.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.17.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.17.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.17.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.17.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.17.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.17.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.17.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.17.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.17.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.17.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.17.output.dense.weight\n",
      "Freezing bert.encoder.layer.17.output.dense.bias\n",
      "Freezing bert.encoder.layer.18.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.18.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.18.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.18.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.18.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.18.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.18.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.18.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.18.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.18.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.18.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.18.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.18.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.18.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.18.output.dense.weight\n",
      "Freezing bert.encoder.layer.18.output.dense.bias\n",
      "Freezing bert.encoder.layer.19.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.19.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.19.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.19.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.19.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.19.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.19.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.19.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.19.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.19.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.19.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.19.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.19.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.19.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.19.output.dense.weight\n",
      "Freezing bert.encoder.layer.19.output.dense.bias\n",
      "Freezing bert.encoder.layer.20.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.20.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.20.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.20.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.20.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.20.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.20.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.20.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.20.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.20.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.20.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.20.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.20.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.20.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.20.output.dense.weight\n",
      "Freezing bert.encoder.layer.20.output.dense.bias\n",
      "Freezing bert.encoder.layer.21.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.21.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.21.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.21.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.21.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.21.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.21.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.21.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.21.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.21.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.21.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.21.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.21.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.21.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.21.output.dense.weight\n",
      "Freezing bert.encoder.layer.21.output.dense.bias\n",
      "Freezing bert.encoder.layer.22.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.22.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.22.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.22.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.22.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.22.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.22.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.22.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.22.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.22.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.22.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.22.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.22.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.22.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.22.output.dense.weight\n",
      "Freezing bert.encoder.layer.22.output.dense.bias\n",
      "Freezing bert.encoder.layer.23.attention.self.query.weight\n",
      "Freezing bert.encoder.layer.23.attention.self.query.bias\n",
      "Freezing bert.encoder.layer.23.attention.self.key.weight\n",
      "Freezing bert.encoder.layer.23.attention.self.key.bias\n",
      "Freezing bert.encoder.layer.23.attention.self.value.weight\n",
      "Freezing bert.encoder.layer.23.attention.self.value.bias\n",
      "Freezing bert.encoder.layer.23.attention.output.dense.weight\n",
      "Freezing bert.encoder.layer.23.attention.output.dense.bias\n",
      "Not freezing bert.encoder.layer.23.PreAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.23.PreAttentionLayerNorm.bias\n",
      "Not freezing bert.encoder.layer.23.PostAttentionLayerNorm.weight\n",
      "Not freezing bert.encoder.layer.23.PostAttentionLayerNorm.bias\n",
      "Freezing bert.encoder.layer.23.intermediate.dense_act.weight\n",
      "Freezing bert.encoder.layer.23.intermediate.dense_act.bias\n",
      "Freezing bert.encoder.layer.23.output.dense.weight\n",
      "Freezing bert.encoder.layer.23.output.dense.bias\n",
      "Freezing bert.pooler.dense_act.weight\n",
      "Freezing bert.pooler.dense_act.bias\n",
      "Not freezing cls.predictions.bias\n",
      "Freezing cls.predictions.transform.dense_act.weight\n",
      "Freezing cls.predictions.transform.dense_act.bias\n",
      "Not freezing cls.predictions.transform.LayerNorm.weight\n",
      "Not freezing cls.predictions.transform.LayerNorm.bias\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0015, -0.0206,  0.0151,  ..., -0.0108, -0.0067, -0.0069],\n",
       "        [ 0.0178, -0.0023, -0.0117,  ..., -0.0180, -0.0257,  0.0064],\n",
       "        [-0.0168, -0.0162,  0.0171,  ..., -0.0160, -0.0260, -0.0039],\n",
       "        ...,\n",
       "        [ 0.0054, -0.0016, -0.0174,  ...,  0.0044,  0.0229,  0.0032],\n",
       "        [ 0.0002, -0.0021,  0.0235,  ...,  0.0542,  0.0058,  0.0511],\n",
       "        [ 0.0250, -0.0197,  0.0013,  ..., -0.0097, -0.0184,  0.0293]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.cls.predictions.decoder.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias',\n",
       " 'transform.dense_act.weight',\n",
       " 'transform.dense_act.bias',\n",
       " 'transform.LayerNorm.weight',\n",
       " 'transform.LayerNorm.bias',\n",
       " 'decoder.weight']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name, p in net.cls.predictions.named_parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cls.predictions.bias'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert.embeddings.word_embeddings.weight',\n",
       " 'bert.embeddings.position_embeddings.weight',\n",
       " 'bert.embeddings.token_type_embeddings.weight',\n",
       " 'bert.encoder.FinalLayerNorm.weight',\n",
       " 'bert.encoder.FinalLayerNorm.bias',\n",
       " 'bert.encoder.layer.0.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.0.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.0.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.0.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.1.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.1.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.1.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.1.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.2.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.2.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.2.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.2.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.3.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.3.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.3.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.3.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.4.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.4.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.4.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.4.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.5.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.5.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.5.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.5.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.6.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.6.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.6.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.6.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.7.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.7.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.7.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.7.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.8.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.8.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.8.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.8.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.9.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.9.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.9.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.9.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.10.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.10.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.10.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.10.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.11.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.11.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.11.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.11.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.12.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.12.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.12.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.12.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.13.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.13.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.13.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.13.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.14.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.14.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.14.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.14.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.15.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.15.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.15.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.15.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.16.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.16.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.16.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.16.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.17.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.17.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.17.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.17.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.18.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.18.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.18.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.18.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.19.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.19.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.19.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.19.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.20.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.20.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.20.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.20.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.21.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.21.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.21.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.21.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.22.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.22.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.22.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.22.PostAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.23.PreAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.23.PreAttentionLayerNorm.bias',\n",
       " 'bert.encoder.layer.23.PostAttentionLayerNorm.weight',\n",
       " 'bert.encoder.layer.23.PostAttentionLayerNorm.bias',\n",
       " 'cls.predictions.bias',\n",
       " 'cls.predictions.transform.dense_act.weight',\n",
       " 'cls.predictions.transform.dense_act.bias',\n",
       " 'cls.predictions.transform.LayerNorm.weight',\n",
       " 'cls.predictions.transform.LayerNorm.bias']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name,p in net.named_parameters() if p.requires_grad]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todos:\n",
    "\n",
    "## freeze attention + mlp layers + let it run\n",
    "## check 2 stitch to be activation preserving\n",
    "## check 4 stitch to be activation preserving for one layer\n",
    "## understand cls for 4 stitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = stitched_model.network.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_input = torch.randint(0, 30528, (1, 512), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(net(rand_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/code/jupyter/get_model.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f706564616e7469635f6b656c64797368222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d65677075613130302d613031227d7d/opt/ml/code/jupyter/get_model.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m out \u001b[39m=\u001b[39m net(rand_input)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f706564616e7469635f6b656c64797368222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d65677075613130302d613031227d7d/opt/ml/code/jupyter/get_model.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(out[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "out = net(rand_input)\n",
    "print(out[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_input = torch.randint(0, 30528, (1, 512), dtype=torch.long)\n",
    "mask = torch.ones_like(rand_input, dtype=torch.bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/code/jupyter/get_model.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f706564616e7469635f6b656c64797368222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6d65677075613130302d613031227d7d/opt/ml/code/jupyter/get_model.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m stitched_model\u001b[39m.\u001b[39;49mnetwork(rand_input, attention_mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'attention_mask'"
     ]
    }
   ],
   "source": [
    "stitched_model.network(rand_input, attention_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30528, 1024)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stitched_model.network.bert.embeddings.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertOnlyMLMHead(\n",
       "  (predictions): BertLMPredictionHead(\n",
       "    (transform): BertPredictionHeadTransform(\n",
       "      (dense_act): LinearActivation(in_features=1024, out_features=1024, bias=True)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=1024, out_features=30528, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stitched_model.network.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30528, 1024)\n",
       "    (position_embeddings): Embedding(512, 1024)\n",
       "    (token_type_embeddings): Embedding(2, 1024)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (FinalLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (18): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (19): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (20): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (21): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (22): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (23): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense_act): LinearActivation(in_features=1024, out_features=1024, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stitched_model.network.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(out[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertLMHeadModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30528, 1024)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (FinalLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (softmax): Softmax(dim=-1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (PreAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (PostAttentionLayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense_act): LinearActivation(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense_act): LinearActivation(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense_act): LinearActivation(in_features=1024, out_features=1024, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=30528, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stitched_model.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not freezing embeddings.word_embeddings.weight\n",
      "Not freezing embeddings.position_embeddings.weight\n",
      "Not freezing embeddings.token_type_embeddings.weight\n",
      "Not freezing encoder.FinalLayerNorm.weight\n",
      "Not freezing encoder.FinalLayerNorm.bias\n",
      "Freezing encoder.layer.0.attention.self.query.weight\n",
      "Freezing encoder.layer.0.attention.self.query.bias\n",
      "Freezing encoder.layer.0.attention.self.key.weight\n",
      "Freezing encoder.layer.0.attention.self.key.bias\n",
      "Freezing encoder.layer.0.attention.self.value.weight\n",
      "Freezing encoder.layer.0.attention.self.value.bias\n",
      "Freezing encoder.layer.0.attention.output.dense.weight\n",
      "Freezing encoder.layer.0.attention.output.dense.bias\n",
      "Not freezing encoder.layer.0.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.0.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.0.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.0.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.0.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.0.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.0.output.dense.weight\n",
      "Freezing encoder.layer.0.output.dense.bias\n",
      "Freezing encoder.layer.1.attention.self.query.weight\n",
      "Freezing encoder.layer.1.attention.self.query.bias\n",
      "Freezing encoder.layer.1.attention.self.key.weight\n",
      "Freezing encoder.layer.1.attention.self.key.bias\n",
      "Freezing encoder.layer.1.attention.self.value.weight\n",
      "Freezing encoder.layer.1.attention.self.value.bias\n",
      "Freezing encoder.layer.1.attention.output.dense.weight\n",
      "Freezing encoder.layer.1.attention.output.dense.bias\n",
      "Not freezing encoder.layer.1.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.1.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.1.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.1.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.1.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.1.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.1.output.dense.weight\n",
      "Freezing encoder.layer.1.output.dense.bias\n",
      "Freezing encoder.layer.2.attention.self.query.weight\n",
      "Freezing encoder.layer.2.attention.self.query.bias\n",
      "Freezing encoder.layer.2.attention.self.key.weight\n",
      "Freezing encoder.layer.2.attention.self.key.bias\n",
      "Freezing encoder.layer.2.attention.self.value.weight\n",
      "Freezing encoder.layer.2.attention.self.value.bias\n",
      "Freezing encoder.layer.2.attention.output.dense.weight\n",
      "Freezing encoder.layer.2.attention.output.dense.bias\n",
      "Not freezing encoder.layer.2.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.2.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.2.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.2.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.2.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.2.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.2.output.dense.weight\n",
      "Freezing encoder.layer.2.output.dense.bias\n",
      "Freezing encoder.layer.3.attention.self.query.weight\n",
      "Freezing encoder.layer.3.attention.self.query.bias\n",
      "Freezing encoder.layer.3.attention.self.key.weight\n",
      "Freezing encoder.layer.3.attention.self.key.bias\n",
      "Freezing encoder.layer.3.attention.self.value.weight\n",
      "Freezing encoder.layer.3.attention.self.value.bias\n",
      "Freezing encoder.layer.3.attention.output.dense.weight\n",
      "Freezing encoder.layer.3.attention.output.dense.bias\n",
      "Not freezing encoder.layer.3.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.3.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.3.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.3.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.3.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.3.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.3.output.dense.weight\n",
      "Freezing encoder.layer.3.output.dense.bias\n",
      "Freezing encoder.layer.4.attention.self.query.weight\n",
      "Freezing encoder.layer.4.attention.self.query.bias\n",
      "Freezing encoder.layer.4.attention.self.key.weight\n",
      "Freezing encoder.layer.4.attention.self.key.bias\n",
      "Freezing encoder.layer.4.attention.self.value.weight\n",
      "Freezing encoder.layer.4.attention.self.value.bias\n",
      "Freezing encoder.layer.4.attention.output.dense.weight\n",
      "Freezing encoder.layer.4.attention.output.dense.bias\n",
      "Not freezing encoder.layer.4.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.4.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.4.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.4.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.4.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.4.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.4.output.dense.weight\n",
      "Freezing encoder.layer.4.output.dense.bias\n",
      "Freezing encoder.layer.5.attention.self.query.weight\n",
      "Freezing encoder.layer.5.attention.self.query.bias\n",
      "Freezing encoder.layer.5.attention.self.key.weight\n",
      "Freezing encoder.layer.5.attention.self.key.bias\n",
      "Freezing encoder.layer.5.attention.self.value.weight\n",
      "Freezing encoder.layer.5.attention.self.value.bias\n",
      "Freezing encoder.layer.5.attention.output.dense.weight\n",
      "Freezing encoder.layer.5.attention.output.dense.bias\n",
      "Not freezing encoder.layer.5.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.5.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.5.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.5.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.5.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.5.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.5.output.dense.weight\n",
      "Freezing encoder.layer.5.output.dense.bias\n",
      "Freezing encoder.layer.6.attention.self.query.weight\n",
      "Freezing encoder.layer.6.attention.self.query.bias\n",
      "Freezing encoder.layer.6.attention.self.key.weight\n",
      "Freezing encoder.layer.6.attention.self.key.bias\n",
      "Freezing encoder.layer.6.attention.self.value.weight\n",
      "Freezing encoder.layer.6.attention.self.value.bias\n",
      "Freezing encoder.layer.6.attention.output.dense.weight\n",
      "Freezing encoder.layer.6.attention.output.dense.bias\n",
      "Not freezing encoder.layer.6.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.6.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.6.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.6.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.6.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.6.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.6.output.dense.weight\n",
      "Freezing encoder.layer.6.output.dense.bias\n",
      "Freezing encoder.layer.7.attention.self.query.weight\n",
      "Freezing encoder.layer.7.attention.self.query.bias\n",
      "Freezing encoder.layer.7.attention.self.key.weight\n",
      "Freezing encoder.layer.7.attention.self.key.bias\n",
      "Freezing encoder.layer.7.attention.self.value.weight\n",
      "Freezing encoder.layer.7.attention.self.value.bias\n",
      "Freezing encoder.layer.7.attention.output.dense.weight\n",
      "Freezing encoder.layer.7.attention.output.dense.bias\n",
      "Not freezing encoder.layer.7.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.7.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.7.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.7.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.7.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.7.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.7.output.dense.weight\n",
      "Freezing encoder.layer.7.output.dense.bias\n",
      "Freezing encoder.layer.8.attention.self.query.weight\n",
      "Freezing encoder.layer.8.attention.self.query.bias\n",
      "Freezing encoder.layer.8.attention.self.key.weight\n",
      "Freezing encoder.layer.8.attention.self.key.bias\n",
      "Freezing encoder.layer.8.attention.self.value.weight\n",
      "Freezing encoder.layer.8.attention.self.value.bias\n",
      "Freezing encoder.layer.8.attention.output.dense.weight\n",
      "Freezing encoder.layer.8.attention.output.dense.bias\n",
      "Not freezing encoder.layer.8.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.8.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.8.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.8.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.8.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.8.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.8.output.dense.weight\n",
      "Freezing encoder.layer.8.output.dense.bias\n",
      "Freezing encoder.layer.9.attention.self.query.weight\n",
      "Freezing encoder.layer.9.attention.self.query.bias\n",
      "Freezing encoder.layer.9.attention.self.key.weight\n",
      "Freezing encoder.layer.9.attention.self.key.bias\n",
      "Freezing encoder.layer.9.attention.self.value.weight\n",
      "Freezing encoder.layer.9.attention.self.value.bias\n",
      "Freezing encoder.layer.9.attention.output.dense.weight\n",
      "Freezing encoder.layer.9.attention.output.dense.bias\n",
      "Not freezing encoder.layer.9.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.9.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.9.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.9.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.9.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.9.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.9.output.dense.weight\n",
      "Freezing encoder.layer.9.output.dense.bias\n",
      "Freezing encoder.layer.10.attention.self.query.weight\n",
      "Freezing encoder.layer.10.attention.self.query.bias\n",
      "Freezing encoder.layer.10.attention.self.key.weight\n",
      "Freezing encoder.layer.10.attention.self.key.bias\n",
      "Freezing encoder.layer.10.attention.self.value.weight\n",
      "Freezing encoder.layer.10.attention.self.value.bias\n",
      "Freezing encoder.layer.10.attention.output.dense.weight\n",
      "Freezing encoder.layer.10.attention.output.dense.bias\n",
      "Not freezing encoder.layer.10.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.10.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.10.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.10.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.10.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.10.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.10.output.dense.weight\n",
      "Freezing encoder.layer.10.output.dense.bias\n",
      "Freezing encoder.layer.11.attention.self.query.weight\n",
      "Freezing encoder.layer.11.attention.self.query.bias\n",
      "Freezing encoder.layer.11.attention.self.key.weight\n",
      "Freezing encoder.layer.11.attention.self.key.bias\n",
      "Freezing encoder.layer.11.attention.self.value.weight\n",
      "Freezing encoder.layer.11.attention.self.value.bias\n",
      "Freezing encoder.layer.11.attention.output.dense.weight\n",
      "Freezing encoder.layer.11.attention.output.dense.bias\n",
      "Not freezing encoder.layer.11.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.11.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.11.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.11.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.11.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.11.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.11.output.dense.weight\n",
      "Freezing encoder.layer.11.output.dense.bias\n",
      "Freezing encoder.layer.12.attention.self.query.weight\n",
      "Freezing encoder.layer.12.attention.self.query.bias\n",
      "Freezing encoder.layer.12.attention.self.key.weight\n",
      "Freezing encoder.layer.12.attention.self.key.bias\n",
      "Freezing encoder.layer.12.attention.self.value.weight\n",
      "Freezing encoder.layer.12.attention.self.value.bias\n",
      "Freezing encoder.layer.12.attention.output.dense.weight\n",
      "Freezing encoder.layer.12.attention.output.dense.bias\n",
      "Not freezing encoder.layer.12.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.12.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.12.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.12.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.12.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.12.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.12.output.dense.weight\n",
      "Freezing encoder.layer.12.output.dense.bias\n",
      "Freezing encoder.layer.13.attention.self.query.weight\n",
      "Freezing encoder.layer.13.attention.self.query.bias\n",
      "Freezing encoder.layer.13.attention.self.key.weight\n",
      "Freezing encoder.layer.13.attention.self.key.bias\n",
      "Freezing encoder.layer.13.attention.self.value.weight\n",
      "Freezing encoder.layer.13.attention.self.value.bias\n",
      "Freezing encoder.layer.13.attention.output.dense.weight\n",
      "Freezing encoder.layer.13.attention.output.dense.bias\n",
      "Not freezing encoder.layer.13.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.13.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.13.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.13.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.13.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.13.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.13.output.dense.weight\n",
      "Freezing encoder.layer.13.output.dense.bias\n",
      "Freezing encoder.layer.14.attention.self.query.weight\n",
      "Freezing encoder.layer.14.attention.self.query.bias\n",
      "Freezing encoder.layer.14.attention.self.key.weight\n",
      "Freezing encoder.layer.14.attention.self.key.bias\n",
      "Freezing encoder.layer.14.attention.self.value.weight\n",
      "Freezing encoder.layer.14.attention.self.value.bias\n",
      "Freezing encoder.layer.14.attention.output.dense.weight\n",
      "Freezing encoder.layer.14.attention.output.dense.bias\n",
      "Not freezing encoder.layer.14.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.14.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.14.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.14.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.14.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.14.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.14.output.dense.weight\n",
      "Freezing encoder.layer.14.output.dense.bias\n",
      "Freezing encoder.layer.15.attention.self.query.weight\n",
      "Freezing encoder.layer.15.attention.self.query.bias\n",
      "Freezing encoder.layer.15.attention.self.key.weight\n",
      "Freezing encoder.layer.15.attention.self.key.bias\n",
      "Freezing encoder.layer.15.attention.self.value.weight\n",
      "Freezing encoder.layer.15.attention.self.value.bias\n",
      "Freezing encoder.layer.15.attention.output.dense.weight\n",
      "Freezing encoder.layer.15.attention.output.dense.bias\n",
      "Not freezing encoder.layer.15.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.15.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.15.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.15.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.15.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.15.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.15.output.dense.weight\n",
      "Freezing encoder.layer.15.output.dense.bias\n",
      "Freezing encoder.layer.16.attention.self.query.weight\n",
      "Freezing encoder.layer.16.attention.self.query.bias\n",
      "Freezing encoder.layer.16.attention.self.key.weight\n",
      "Freezing encoder.layer.16.attention.self.key.bias\n",
      "Freezing encoder.layer.16.attention.self.value.weight\n",
      "Freezing encoder.layer.16.attention.self.value.bias\n",
      "Freezing encoder.layer.16.attention.output.dense.weight\n",
      "Freezing encoder.layer.16.attention.output.dense.bias\n",
      "Not freezing encoder.layer.16.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.16.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.16.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.16.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.16.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.16.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.16.output.dense.weight\n",
      "Freezing encoder.layer.16.output.dense.bias\n",
      "Freezing encoder.layer.17.attention.self.query.weight\n",
      "Freezing encoder.layer.17.attention.self.query.bias\n",
      "Freezing encoder.layer.17.attention.self.key.weight\n",
      "Freezing encoder.layer.17.attention.self.key.bias\n",
      "Freezing encoder.layer.17.attention.self.value.weight\n",
      "Freezing encoder.layer.17.attention.self.value.bias\n",
      "Freezing encoder.layer.17.attention.output.dense.weight\n",
      "Freezing encoder.layer.17.attention.output.dense.bias\n",
      "Not freezing encoder.layer.17.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.17.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.17.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.17.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.17.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.17.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.17.output.dense.weight\n",
      "Freezing encoder.layer.17.output.dense.bias\n",
      "Freezing encoder.layer.18.attention.self.query.weight\n",
      "Freezing encoder.layer.18.attention.self.query.bias\n",
      "Freezing encoder.layer.18.attention.self.key.weight\n",
      "Freezing encoder.layer.18.attention.self.key.bias\n",
      "Freezing encoder.layer.18.attention.self.value.weight\n",
      "Freezing encoder.layer.18.attention.self.value.bias\n",
      "Freezing encoder.layer.18.attention.output.dense.weight\n",
      "Freezing encoder.layer.18.attention.output.dense.bias\n",
      "Not freezing encoder.layer.18.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.18.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.18.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.18.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.18.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.18.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.18.output.dense.weight\n",
      "Freezing encoder.layer.18.output.dense.bias\n",
      "Freezing encoder.layer.19.attention.self.query.weight\n",
      "Freezing encoder.layer.19.attention.self.query.bias\n",
      "Freezing encoder.layer.19.attention.self.key.weight\n",
      "Freezing encoder.layer.19.attention.self.key.bias\n",
      "Freezing encoder.layer.19.attention.self.value.weight\n",
      "Freezing encoder.layer.19.attention.self.value.bias\n",
      "Freezing encoder.layer.19.attention.output.dense.weight\n",
      "Freezing encoder.layer.19.attention.output.dense.bias\n",
      "Not freezing encoder.layer.19.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.19.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.19.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.19.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.19.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.19.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.19.output.dense.weight\n",
      "Freezing encoder.layer.19.output.dense.bias\n",
      "Freezing encoder.layer.20.attention.self.query.weight\n",
      "Freezing encoder.layer.20.attention.self.query.bias\n",
      "Freezing encoder.layer.20.attention.self.key.weight\n",
      "Freezing encoder.layer.20.attention.self.key.bias\n",
      "Freezing encoder.layer.20.attention.self.value.weight\n",
      "Freezing encoder.layer.20.attention.self.value.bias\n",
      "Freezing encoder.layer.20.attention.output.dense.weight\n",
      "Freezing encoder.layer.20.attention.output.dense.bias\n",
      "Not freezing encoder.layer.20.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.20.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.20.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.20.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.20.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.20.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.20.output.dense.weight\n",
      "Freezing encoder.layer.20.output.dense.bias\n",
      "Freezing encoder.layer.21.attention.self.query.weight\n",
      "Freezing encoder.layer.21.attention.self.query.bias\n",
      "Freezing encoder.layer.21.attention.self.key.weight\n",
      "Freezing encoder.layer.21.attention.self.key.bias\n",
      "Freezing encoder.layer.21.attention.self.value.weight\n",
      "Freezing encoder.layer.21.attention.self.value.bias\n",
      "Freezing encoder.layer.21.attention.output.dense.weight\n",
      "Freezing encoder.layer.21.attention.output.dense.bias\n",
      "Not freezing encoder.layer.21.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.21.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.21.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.21.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.21.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.21.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.21.output.dense.weight\n",
      "Freezing encoder.layer.21.output.dense.bias\n",
      "Freezing encoder.layer.22.attention.self.query.weight\n",
      "Freezing encoder.layer.22.attention.self.query.bias\n",
      "Freezing encoder.layer.22.attention.self.key.weight\n",
      "Freezing encoder.layer.22.attention.self.key.bias\n",
      "Freezing encoder.layer.22.attention.self.value.weight\n",
      "Freezing encoder.layer.22.attention.self.value.bias\n",
      "Freezing encoder.layer.22.attention.output.dense.weight\n",
      "Freezing encoder.layer.22.attention.output.dense.bias\n",
      "Not freezing encoder.layer.22.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.22.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.22.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.22.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.22.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.22.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.22.output.dense.weight\n",
      "Freezing encoder.layer.22.output.dense.bias\n",
      "Freezing encoder.layer.23.attention.self.query.weight\n",
      "Freezing encoder.layer.23.attention.self.query.bias\n",
      "Freezing encoder.layer.23.attention.self.key.weight\n",
      "Freezing encoder.layer.23.attention.self.key.bias\n",
      "Freezing encoder.layer.23.attention.self.value.weight\n",
      "Freezing encoder.layer.23.attention.self.value.bias\n",
      "Freezing encoder.layer.23.attention.output.dense.weight\n",
      "Freezing encoder.layer.23.attention.output.dense.bias\n",
      "Not freezing encoder.layer.23.PreAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.23.PreAttentionLayerNorm.bias\n",
      "Not freezing encoder.layer.23.PostAttentionLayerNorm.weight\n",
      "Not freezing encoder.layer.23.PostAttentionLayerNorm.bias\n",
      "Freezing encoder.layer.23.intermediate.dense_act.weight\n",
      "Freezing encoder.layer.23.intermediate.dense_act.bias\n",
      "Freezing encoder.layer.23.output.dense.weight\n",
      "Freezing encoder.layer.23.output.dense.bias\n",
      "Freezing pooler.dense_act.weight\n",
      "Freezing pooler.dense_act.bias\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertOnlyMLMHead(\n",
       "  (predictions): BertLMPredictionHead(\n",
       "    (transform): BertPredictionHeadTransform(\n",
       "      (dense_act): LinearActivation(in_features=1024, out_features=1024, bias=True)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([1024]), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): Linear(in_features=1024, out_features=30528, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert', 'cls']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k,v in net.named_children()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.bert.encoder.layer[0].intermediate.dense_act.weight.requires_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
